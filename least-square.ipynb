{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "###  Linear Regression Analytical Problem\nConsider the following training data.\n\n| X1 | X2 | Y |\n| -- | -- | -- |\n| 0 | 0 | 0 |\n| 0 | 1 | 1.5 |\n| 1 | 0 | 2 |\n| 1 | 1 | 2.5 |\nSuppose our data comes from a model y = $\u03b8_{0}$ +$\u03b8_{1}$x1 +$\u03b8_{2}$x2 for unknown constants $\u03b8_{0}$,$\u03b8_{1}$,$\u03b8_{2}$. \nin the following code we use least squares linear regression to find an estimate of $\u03b8_{0}$,$\u03b8_{1}$,$\u03b8_{2}$.(the code is implemeted from scratch with no machine learning libraries"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "***First:***\nWe have to add the dummy features (all 1s) as column x0 to the given table (the given data).\nthe data become:\n\n| X0 | X1 | X2 | Y  |\n| -- | -- | -- | -- |\n| 1 | 0 | 0 | 0 |\n| 1 | 0 | 1 | 1.5 |\n| 1 | 1 | 0 | 2 |\n| 1 | 1 | 1 | 2.5 |\n\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import pandas as pd\n\n#we convert the data to a dataframe to perform our calculations\nr = [[1, 0, 0, 0], [1, 0, 1, 1.5], [1, 1, 0, 2], [1, 1, 1, 2.5]]\ndata1 = pd.DataFrame(r, columns=['x0', 'x1', 'x2', 'Y'])\ndata1",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 1,
                    "data": {
                        "text/plain": "   x0  x1  x2    Y\n0   1   0   0  0.0\n1   1   0   1  1.5\n2   1   1   0  2.0\n3   1   1   1  2.5",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x0</th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#Initializing w0, w1, w2\n\u03b80 = 0\n\u03b81 = 0\n\u03b82 = 0\n\n#Setting up the learning rate\nlearning = 0.1\n\n#Stopping the Iteration when SSE < 0.5\nthreshold = 0.5\n\niteration = 0\n\nfor i in range(100):\n    #Calculating y_hat1, y_hat2, y_hat3, y_hat4\n    y_hat1 = \u03b80 + \u03b81 * data1.iloc[0, 1] + \u03b82 * data1.iloc[0, 2]\n    y_hat2 = \u03b80 + \u03b81 * data1.iloc[1, 1] + \u03b82 * data1.iloc[1, 2]\n    y_hat3 = \u03b80 + \u03b81 * data1.iloc[2, 1] + \u03b82 * data1.iloc[2, 2]\n    y_hat4 = \u03b80 + \u03b81 * data1.iloc[3, 1] + \u03b82 * data1.iloc[3, 2]\n    \n    #Calculating the SSE\n    SSE = (y_hat1 - data1.iloc[0, 3])**2 +\\\n          (y_hat2 - data1.iloc[1, 3])**2 +\\\n          (y_hat3 - data1.iloc[2, 3])**2 +\\\n          (y_hat4 - data1.iloc[3, 3])**2\n    print(f\"iteration: {i}, the SSE is: {SSE}\")\n    \n    #Checking if the SSE < 0.5\n    if SSE < threshold:\n        \n        #If the SSE < 0.5 we are done\n        print(f\"{SSE} < {threshold}, therefore reached stopping condition\")\n        break\n    #If  the SSE > 0.5 we proceed by calculating the followings\n    else:\n        #Calculating s0\n        s0 = (y_hat1 - data1.iloc[0, 3]) +\\\n              (y_hat2 - data1.iloc[1, 3]) +\\\n              (y_hat3 - data1.iloc[2, 3]) +\\\n              (y_hat4 - data1.iloc[3, 3])\n        \n        #Calculating s1\n        s1 = (y_hat1 - data1.iloc[0, 3]) * data1.iloc[0, 1]  +\\\n              (y_hat2 - data1.iloc[1, 3]) * data1.iloc[1, 1] +\\\n              (y_hat3 - data1.iloc[2, 3]) * data1.iloc[2, 1] +\\\n              (y_hat4 - data1.iloc[3, 3]) * data1.iloc[3, 1]\n        \n        #Calculating s2\n        s2 = (y_hat1 - data1.iloc[0, 3]) * data1.iloc[0, 2]  +\\\n              (y_hat2 - data1.iloc[1, 3]) * data1.iloc[1, 2] +\\\n              (y_hat3 - data1.iloc[2, 3]) * data1.iloc[2, 2] +\\\n              (y_hat4 - data1.iloc[3, 3]) * data1.iloc[3, 2]\n        \n        #Calculating \u03b80\n        \u03b80 = \u03b80 - learning *(1/4) * (s0)\n        #Calculating \u03b81\n        \u03b81 = \u03b81 - learning *(1/4) * (s1)\n        #Calculating \u03b82\n        \u03b82 = \u03b82 - learning *(1/4) * (s2)\n        \n        #Printing \u03b80, \u03b81, \u03b82 after each iteration\n        print(f\"the weights are: {\u03b80}, {\u03b81}, {\u03b82}\")\n        iteration += 1\nprint(f\"----------------------\") \nprint(\"The final results are: \")\nprint(f\"\u03b80 is {\u03b80}\")\nprint(f\"\u03b81 is {\u03b81}\")\nprint(f\"\u03b82 is {\u03b82}\")\nprint(f\"The number of iterations is: {iteration}\")",
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "iteration: 0, the SSE is: 12.5\nthe weights are: 0.15000000000000002, 0.1125, 0.1\niteration: 1, the SSE is: 9.1728125\nthe weights are: 0.27437500000000004, 0.209375, 0.1846875\niteration: 2, the SSE is: 6.8124677734375\nthe weights are: 0.377234375, 0.29307031250000004, 0.2565\niteration: 3, the SSE is: 5.135755022094727\nthe weights are: 0.46203242187500004, 0.36564257812500006, 0.3174865234375\niteration: 4, the SSE is: 3.9425001122264858\nthe weights are: 0.5316727246093751, 0.42882166503906255, 0.36936951171875\niteration: 5, the SSE is: 3.091216692032363\nthe weights are: 0.588595893310547, 0.48406270776367194, 0.41359685827636716\niteration: 6, the SSE is: 2.481897246052765\nthe weights are: 0.6348533256774903, 0.5325898562530518, 0.45138565300292965\niteration: 7, the SSE is: 2.043850405345834\nthe weights are: 0.6721692176469422, 0.5754330558314514, 0.48375895766258237\niteration: 8, the SSE is: 1.727104773565934\nthe weights are: 0.7019926952075463, 0.6134589682159672, 0.5115767225013199\niteration: 9, the SSE is: 1.4963334410569629\nthe weights are: 0.7255416411509272, 0.6473969669822586, 0.5355617774104774\niteration: 10, the SSE is: 1.3265600327389695\nthe weights are: 0.7438395398161977, 0.6778609921403373, 0.5563216823078507\niteration: 11, the SSE is: 1.2001238816777362\nthe weights are: 0.7577464521121685, 0.7053679234848144, 0.5743670963981399\niteration: 12, the SSE is: 1.1045351087207955\nthe weights are: 0.767985055906804, 0.7303530272950117, 0.5901272208855041\niteration: 13, the SSE is: 1.0309586567988103\nthe weights are: 0.7751625379070978, 0.7531829426127834, 0.6039627813635133\niteration: 14, the SSE is: 0.9731428486412371\nthe weights are: 0.7797889979175732, 0.7741665990527016, 0.6161769418346632\niteration: 15, the SSE is: 0.9266621183943478\nthe weights are: 0.7822929210814477, 0.7935643956583213, 0.6270244798707338\niteration: 16, the SSE is: 0.8883817906928031\nthe weights are: 0.7830341851968502, 0.8115959178245645, 0.6367194999316668\niteration: 17, the SSE is: 0.8560797954472441\nthe weights are: 0.7823149957893536, 0.8284464251752022, 0.6454419177296268\niteration: 18, the SSE is: 0.8281792996545956\nthe weights are: 0.7803890790651767, 0.8442723061837337, 0.6533429114242977\niteration: 19, the SSE is: 0.8035597318275057\nthe weights are: 0.7774694102782576, 0.8592056641356808, 0.6605495042452306\niteration: 20, the SSE is: 0.781423211927337\nthe weights are: 0.7737347108313862, 0.8733581728088531, 0.6671684169156642\niteration: 21, the SSE is: 0.7612001403039562\nthe weights are: 0.7693349102620217, 0.8868243182039495, 0.6732893062080904\niteration: 22, the SSE is: 0.7424824631774826\nthe weights are: 0.7643957380152175, 0.8996841241254486, 0.678987487429486\niteration: 23, the SSE is: 0.7249764992472452\nthe weights are: 0.7590225836359491, 0.9120054438326781, 0.6843262230541146\niteration: 24, the SSE is: 0.7084695917258512\nthe weights are: 0.7533037419280145, 0.9238458868828939, 0.6893586466237944\niteration: 25, the SSE is: 0.6928065319936838\nthe weights are: 0.7473131410598787, 0.9352544392767537, 0.6941293800241316\niteration: 26, the SSE is: 0.6778728897753942\nthe weights are: 0.7411126359888466, 0.9462728257593188, 0.6986758929880122\niteration: 27, the SSE is: 0.6635832248760701\nthe weights are: 0.7347539364525953, 0.9569366553472103, 0.7030296458951864\niteration: 28, the SSE is: 0.649872749293813\nthe weights are: 0.728280227745216, 0.9672763846098403, 0.707217050394117\niteration: 29, the SSE is: 0.6366914281879992\nthe weights are: 0.7217275332204965, 0.9773181277322346, 0.7112602768719043\niteration: 30, the SSE is: 0.6239998047856518\nthe weights are: 0.7151258596682399, 0.9870843377628005, 0.7151779331739784\niteration: 31, the SSE is: 0.611766043937413\nthe weights are: 0.708500160154577, 0.996594379561899, 0.7189856350877974\niteration: 32, the SSE is: 0.5999638371934789\nthe weights are: 0.7018711434066345, 1.0058650116988803, 0.7226964858366313\niteration: 33, the SSE is: 0.58857091698371\nthe weights are: 0.6952559541891955, 1.0149107917976887, 0.726321479081996\niteration: 34, the SSE is: 0.5775680014945955\nthe weights are: 0.6886687452262917, 1.0237444175212946, 0.7298698376234942\niteration: 35, the SSE is: 0.5669380441426873\nthe weights are: 0.6821211579464231, 1.0323770134433279, 0.7333492980429726\niteration: 36, the SSE is: 0.5566656985131765\nthe weights are: 0.6756227265774658, 1.040818372422766, 0.7367663499074195\niteration: 37, the SSE is: 0.546736935761067\nthe weights are: 0.6691812178032099, 1.049077158725069, 0.7401264367726061\niteration: 38, the SSE is: 0.5371387699396099\nthe weights are: 0.6628029162480051, 1.0571610789793398, 0.7434341250756886\niteration: 39, the SSE is: 0.5278590597729765\nthe weights are: 0.6564928644204532, 1.0650770260910802, 0.7466932460350204\niteration: 40, the SSE is: 0.5188863646153333\nthe weights are: 0.6502550643721028, 1.072831200414628, 0.7499070148599697\niteration: 41, the SSE is: 0.5102098388588363\nthe weights are: 0.6440926471711627, 1.0804292118037924, 0.7530781308880004\niteration: 42, the SSE is: 0.5018191536616682\nthe weights are: 0.6380080153194567, 1.0878761655828446, 0.7562088616899474\niteration: 43, the SSE is: 0.4937044381247209\n0.4937044381247209 < 0.5, therefore reached stopping condition\n----------------------\nThe final results are: \n\u03b80 is 0.6380080153194567\n\u03b81 is 1.0878761655828446\n\u03b82 is 0.7562088616899474\nThe number of iterations is: 43\n",
                    "name": "stdout"
                }
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.12",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}